\documentclass{article}
\usepackage{listings}
\usepackage{color}
\usepackage{float}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
	frame=single,
	language=C,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	captionpos=b,
	basicstyle={\small\ttfamily},
	numbers=left,
	numbersep=5pt,
	%numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4
}

\input kvmacros
\sloppy
\begin{document}

\title{Programming Assignment 3 - Matrix Transpose}
\author{\textit{Ming Zhao}\\\textit{Hu Hu}\\\textit{Team3}}
\date{\today}
{\let\newpage\relax\maketitle}
\maketitle

\section{Introduction}
\quad Machine learning is getting more and more popular and the GPU is becoming more and more important because its performance in matrix handling is good.

In this project, we will implement 5 kinds of matrix transpose program: naive CPU, naive GPU, shared memory GPU,  matrix transpose without bank conflict GPU, loop unrolled GPU. I will explain how I implement this 5 kinds of matrix transpose programs later. And we use Pi cluster to test our program.

\section{Implementation of Matrix Transpose}
In this part, we will discuss 5 different kinds of matrix transpose implementation, i.e. naive CPU, naive GPU, shared memory, shared memory without bank conflict and loop unrolling.
\subsection{Naive CPU}
The naive CPU version of matrix transpose is quite easy. The key idea is that we use a nested loop to assign \verb|a[i][j]| to \verb|b[j][i]|. Since it is quite easy, I will just show how I implement it as follows:
\begin{lstlisting}[language=C, caption=naive CPU]
void naiveCPU(float *src, float *dst, int M, int N) {   
    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < M; ++j) {
	        dst[i*N+j] = src[j*N+i];
        }
    }
}
\end{lstlisting}

\subsection{Naive GPU}
The naive GPU version is also not very difficult. We exploit the parallelism by simply translate the naive CPU version to GPU version. What we need to care about is that the function needs to be annotated by \verb|__global__| so that \verb|nvcc| can know that this function should be executed in GPU. Also, before call the matrix transpose function, we need to call \verb|cudaMemcpy| to copy data in host memory to device memory.
\begin{lstlisting}[caption=Naive GPU]
__global__ void matrixTranspose(float *_a, float *_b, int cols,int rows)
{
	int i = blockIdx.y * blockDim.y + threadIdx.y; // row
	int j = blockIdx.x * blockDim.x + threadIdx.x; // col
	
	int index_in = i * cols + j;
	int index_out = j * rows + i;
	
	_b[index_out] = _a[index_in];
}
\end{lstlisting}

\subsection{Shared Memory GPU}
In the shared memory GPU implementation, we divide the whole matrix of size $1024\times 1024$ to $D\times D$ smaller matrices where we make $D$ to be 16. In the CUDA function, we declare a shared memory space \verb|__shared__ float mat[P][P]|. Then we use CUDA primitive \verb|_synctheads()| to wait for all threads in a particular block. Then the GPU begins to transpose the matrix in shared memory to destination submatrix.
\begin{lstlisting}[caption=Shared Memory GPU]
__global__ void matrixTransposeShared(float *_a, float *_b, int cols, int rows)
{
	__shared__ float mat[P][P];
	int bx = blockIdx.x * blockDim.x;
	int by = blockIdx.y * blockDim.y;
	int i = by + threadIdx.y; int j = bx + threadIdx.x;
	int ti = bx + threadIdx.y; int tj = by + threadIdx.x;
	
	if (i < rows && j < cols)
		mat[threadIdx.x][threadIdx.y] = _a[i*cols + j];
	__syncthreads();
	if (tj < cols && ti < rows)
		_b[ti*rows+tj] = mat[threadIdx.y][threadIdx.x];
}
\end{lstlisting} 

\subsection{Shared Memory without Bank Conflicts GPU}
Though eliminate bank conflict seems difficult at first thought, it can be accomplished by simply change the colon size by 1. Every other things will keep the same.
\begin{lstlisting}[caption=Shared Memory without Bank Conflicts GPU]
__global__ void matrixTransposeSharedwBC(float *_a, float *_b, int cols, int rows)
{
	__shared__ float mat[P][P+1];
	int bx = blockIdx.x * blockDim.x;
	int by = blockIdx.y * blockDim.y;
	int i = by + threadIdx.y; int j = bx + threadIdx.x;
	int ti = bx + threadIdx.y; int tj = by + threadIdx.x;
	
	if (i < rows && j < cols)
		mat[threadIdx.x][threadIdx.y] = _a[i*cols + j];
	__syncthreads();
	if (tj < cols && ti < rows)
		_b[ti*rows+tj] = mat[threadIdx.y][threadIdx.x];
}
\end{lstlisting}

\subsection{Loop Unrolling GPU}
Loop unrolling is a common optimization at compile time, which can efficiently decrease the overhead of branch predication and make GPU more pipelined. CUDA provides convenient compile time macros unrolling loops. What we need to do is simply add the \verb|#progma unroll| macro before each for loop.
\begin{lstlisting}[caption=Loop Unrolling GPU]
__global__ void matrixTransposeUnrolled(float *_a, float *_b, int cols, int rows)
{
	__shared__ float mat[P][P+1];
	int x = blockIdx.x * P + threadIdx.x;
	int y = blockIdx.y * P + threadIdx.y;
	
	#pragma unroll
	for (int k = 0; k < P; k += 8) {
		if (x < rows && y+k < cols)
			mat[threadIdx.y+k][threadIdx.x] = _a[(y+k)*rows + x];
	}
	
	__syncthreads();
	
	x = blockIdx.y * P + threadIdx.x;
	y = blockIdx.x * P + threadIdx.y;
	#pragma unroll
	for (int k = 0; k < P; k += 8) {
		if (x < cols && y+k < rows)
			_b[(y+k)*cols + x] = mat[threadIdx.x][threadIdx.y+k];
	}
}
\end{lstlisting}


That's all for our discussion on implementation of different kinds of matrix transpose. Since this is only a introductory project on CUDA, I think it's more important for us to get a rough idea of GPU performance and let's go to discuss the performance of each implementation.

\section{How to time the kernel execution}
To test the running time of each CUDA function, we use CUDA's built-in \verb|Event| related functions, which can get the running time of a CUDA event accurately. The common work flow is as follows:
\begin{lstlisting}[caption=Workflow of Time Measuring]
cudaEventRecord(tStart, 0);
// execute a CUDA event for several times
cudaEventRecord(tEnd, 0);
cudaEventSynchronize(tEnd);
cudaEventElapsedTime(&duration, tStart, tEnd);
\end{lstlisting}
In this way, we can accurately estimate the time of a single matrix transpose operation by repeating it for several times. Immediately after we call \verb|cudaEventRecord(tStart, 0)|, we begin to execute CUDA kernel for say \verb|N| times. When the execution is done, we call \verb|cudaEventRecord(tEnd, 0)|. Note here that we need to call an extra \verb|cudaEventSynchronized(tEnd)| since in CUDA events happen asynchronously and we need to call this function to make our timing accurate. Lastly, we calculate the difference of these two events can store the result to a float number by \verb|cudaEventElapsedTime(&duration, tStart, tEnd)|.

\section{Hardware Information}
To fetch the hardware information when we execute CUDA program in Pi supercomputer, the TA has already given us a CUDA script. What we need to do is compile the CUDA program to binary and submit to Pi. I queried two kinds of GPU on Pi: K40 and K80 and the GPU information is listed as follows:
\begin{verbatim}
Device 0: "Tesla K40c"
  CUDA Driver Version / Runtime Version          7.5 / 7.5
  CUDA Capability Major/Minor version number:    3.5
  Total amount of global memory:                 11520 MBytes (12079136768 bytes)
  (15) Multiprocessors, (192) CUDA Cores/MP:     2880 CUDA Cores
  GPU Max Clock rate:                            745 MHz (0.75 GHz)
  Memory Clock rate:                             3004 Mhz
  Memory Bus Width:                              384-bit
  L2 Cache Size:                                 1572864 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 
  		3D=(4096, 4096, 4096)
  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 8 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >
\end{verbatim}

\begin{verbatim}
Device 0: "Tesla K80"
  CUDA Driver Version / Runtime Version          7.5 / 7.5
  CUDA Capability Major/Minor version number:    3.7
  Total amount of global memory:                 11520 MBytes (12079136768 bytes)
  (13) Multiprocessors, (192) CUDA Cores/MP:     2496 CUDA Cores
  GPU Max Clock rate:                            824 MHz (0.82 GHz)
  Memory Clock rate:                             2505 Mhz
  Memory Bus Width:                              384-bit
  L2 Cache Size:                                 1572864 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 
  			3D=(4096, 4096, 4096)
  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 8 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >
\end{verbatim}
Looking into the difference between K40C and K80 and  I find that K80 has higher GPU clock rate but lower memory clock rate.

\section{Performance \& Analysis}

The following two tables shows the result of different implementations on different hardwares.
\begin{table}[H]
	\begin{tabular}[t]{c|cccc}
&	Time(ms)&Bandwidth(GB/s)&Step Speedup & Speedup vs CPU \\\hline\hline
naive CPU &	5.372	&	1.56	&  &  \\ \hline 
naive GPU &	0.130 &	64.53	&	41.32  &41.32  \\ \hline
Shared Memory &	0.095&	88.30	&	1.37  & 56.55 \\ \hline
Shared Mem without Bank Conflict & 0.067&	125.20		&1.42  & 80.18 \\ \hline
Loop Unrolling & 0.054	&155.34	&1.24  & 99.48 \\ 
	\end{tabular}\caption{K40}
\end{table}
\begin{table}[H]
	\begin{tabular}[t]{c|cccc}
		&	Time(ms)&Bandwidth(GB/s)&Step Speedup & Speedup vs CPU \\\hline\hline
		naive CPU &	5.372	&	1.56&  &  \\ \hline 
		naive GPU &	0.199 &	 42.10 &	26.99  &26.99 \\ \hline
		Shared Memory &	0.145&57.68	&	1.37 & 39.02 \\ \hline
		Shared Mem without Bank Conflict & 0.095&	88.26	& 1.53 & 59.36 \\ \hline
		Loop Unrolling & 0.072&	116.50&1.32  & 78.59 \\ 
	\end{tabular}\caption{K80}
\end{table}
From the above tables, we can conclude that:
\begin{enumerate}
	\item Though K80 has higher GPU rate, it executes slower than K40. But I don't know the exact reason.
	\item We gain a huge performance gain of about 30-40 times by moving from CPU to GPU.
	\item For other methods we use to make it quicker, we gain about 1.3-1.5 times performance improvement.
	\item Loop unrolling is the best method to transpose matrix. For K40, we can get 155.32 GB/s bandwidth, and for K80, we can get 116.50 GB/s bandwidth.
\end{enumerate}



\section{Conclusion}
After having testing the performance of so many matrix-transpose methods, we can get the conclusion that loop unrolling is the best way. And it is seen that GPU is much better than CPU in dealing with matrix handling. And it is useful in machine learning.

And actually CUDA is really not a very hard way to use in parallel programming.

\section{Acknowledgement}
Thanks Prof. Deng for guidance on CUDA programming and TAs' help for configuring cluster and kindly answering our questions. And thanks to our classmates to offer help.
\end{document}
